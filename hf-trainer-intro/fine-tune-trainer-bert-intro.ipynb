{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a805dc3-1632-4b30-8065-52074ac728cf",
   "metadata": {},
   "source": [
    "# Fine-Tuning Transformers Models with HuggingFace Trainer\n",
    "In this example we'll fine-tune [BERT](https://huggingface.co/google-bert/bert-base-cased), with the [IMBD dataset](https://huggingface.co/datasets/imdb) for a Text Classification use-case using the [Trainer class](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).\n",
    "\n",
    "- Setup: <b>conda_python3 kernel</b> and <b>ml.g5.12xlarge</b> SageMaker Classic Notebook Instances. NOTE: You can definitely use a smaller instance as well, training will just take longer. Recommend using SM Training Jobs, we'll cover that in next NB.\n",
    "\n",
    "### Additional Credits & Resources\n",
    "- Trainer Documentation: https://huggingface.co/docs/transformers/en/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ed5a01-1c4b-4353-a391-31eb6c05d3af",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13270d1-dc5a-4816-804b-d1d8a9030d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers[torch] accelerate>=0.26.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b727f4-f6d9-470f-98cb-036fc5a496c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import evaluate\n",
    "import transformers\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b66b4a-4b03-4775-8de7-41cb7bd0249b",
   "metadata": {},
   "source": [
    "## Load & Tokenize Dataset\n",
    "You can utilize built-in/provided HF datasets via their API: https://huggingface.co/docs/datasets/en/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7177854-04e9-4098-85ec-dd2bb389f2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "test_dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "test_subset = test_dataset.select(range(100)) # we will take a subset of the data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eee140-3159-4fac-9bdc-a8cf4bc00f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# tokenize text data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_subset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22299c1d-86d4-4c04-aca6-bde3e1b5024a",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "Load the model and prepare Training Arguments and parameters such as epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b4484e-66ae-4d48-9828-e1694f8a3a02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df4de4-5b00-44dc-a255-f8ecd2933f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  num_train_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d2dcd-fc0f-4a4b-8ab8-7d4ba3214ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# eval function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4fb4f-3925-4ca3-bd52-8dc67b8d3a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test, #using test as eval\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab3c570-2ae1-45a6-9b85-f0f67284a0d9",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c1e0d-be2e-4691-9909-b56a895a91a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e16e34-29ff-4948-9949-720ed19645e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea457a7-c8b7-48d1-9b73-93744ffac645",
   "metadata": {},
   "source": [
    "## Inference & Save Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e3c9d-dcb3-4de4-8cb9-1c1f2e112182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.predict(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e047029-0fc5-47f3-bc57-f054dbb2fa15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./custom_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454642c-f132-4ab0-8905-a6dddff74a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"custom_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072bf66c-c8e6-477f-ba8a-a892434a7a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoding = tokenizer(\"I am super happy\", return_tensors=\"pt\")\n",
    "res = loaded_model(**encoding)\n",
    "predicted_label_classes = res.logits.argmax(-1)\n",
    "predicted_label_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a62a19-cf20-4b0e-b16b-7009a6ff84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_payloads = [\"I am super happy\", \"I am super sad\", \"I am really upset\"]\n",
    "for payload in samp_payloads:\n",
    "    encoding = tokenizer(payload, return_tensors=\"pt\")\n",
    "    res = loaded_model(**encoding)\n",
    "    predicted_label_classes = res.logits.argmax(-1)\n",
    "    print(predicted_label_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
