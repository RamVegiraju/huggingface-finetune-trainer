{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f265ac79-4750-4e6f-bd6a-ebb73a4ae080",
   "metadata": {},
   "source": [
    "# Utilizing HuggingFace SFTTrainer for QLORA/PEFT Training\n",
    "In this example we'll explore leveraging the HuggingFace SFTTrainer class to conduct fine-tuning of a sample [Llama 3-1-8B model](https://huggingface.co/meta-llama/Llama-3.1-8B?library=transformers) utilizing [PEFT](https://huggingface.co/docs/peft/en/index).\n",
    "\n",
    "This example builds on the Trainer HF Demo from earlier: https://www.youtube.com/watch?v=wGKU46ZNFnw&t=805s. We expland into training LLMs leveraging HuggingFace's different Trainer classes from the TRL library: https://github.com/huggingface/trl. [TRL](https://github.com/huggingface/trl) supports different types of fine-tuning techniques such as Supervised Fine-Tuning, GRPO, DPO, and more. In this example we utilize the SFTTrainer to conduct PEFT/QLORA based fine-tuning of a [Llama 3.1-8b model](https://huggingface.co/meta-llama/Llama-3.1-8B).\n",
    "\n",
    "### Setting\n",
    "In a SageMaker AI Classic NB Instance, utilizing conda_python3 kernel and g5.16xlarge (might be overkill for this model, can reduce).\n",
    "\n",
    "### Prerequisites\n",
    "Ensure you have a HF Access Token: https://huggingface.co/docs/hub/en/security-tokens and have requested access to the Llama3.1 model linked above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfdee4-f188-48a3-a900-9d31d56740b9",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We generate a mock dataset here with dummy questions about myself, you can substitute this with your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4488a1-37d0-4e52-9926-11bb647a3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft transformers[torch] trl bitsandbytes -U --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ad774-7301-480f-afdd-4558a5c63f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b129c811-79c0-4b79-a8c3-16052434d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ensure you have requested access for Llama 3.1-8B as well\n",
    "os.environ[\"HF_TOKEN\"] = \"Enter HF Hub Token\"\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = os.environ[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4680435-f734-4cc0-aa94-04068a03fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF hub ID\n",
    "BASE_MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Push adapter artifacts post training\n",
    "OUTPUT_DIR = \"./qlora-peft-output\"\n",
    "ADAPTER_DIR = os.path.join(OUTPUT_DIR, \"adapter\")\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Synthetic \"about me\" dataset (1000 rows)\n",
    "# This dataset might not make full sense due to the random matching, but gives a general idea of my profession and interests to train on\n",
    "# -----------------------------\n",
    "base_facts = [\n",
    "    \"The user is a senior machine learning engineer at a large cloud company.\",\n",
    "    \"The user specializes in building and deploying large language models in production.\",\n",
    "    \"The user lives in a big US city and enjoys boxing, basketball, and tennis.\",\n",
    "    \"The user creates technical YouTube videos about ML infrastructure and LLM serving.\",\n",
    "    \"The user works with services similar to Amazon SageMaker and managed model hosting.\",\n",
    "    \"The user likes combining fitness, combat sports, and engineering in their daily routine.\",\n",
    "    \"The user often helps other engineers optimize GPU usage and model throughput.\",\n",
    "    \"The user enjoys experimenting with multi-adapter inference and LoRA fine-tuning.\",\n",
    "    \"The user is training for an amateur boxing fight while working full-time as an engineer.\",\n",
    "    \"The user prefers concise, practical explanations with real-world deployment context.\",\n",
    "]\n",
    "\n",
    "qa_templates = [\n",
    "    lambda f: f\"\"\"### Instruction:\n",
    "Answer the question about the user based on the known facts.\n",
    "\n",
    "### Input:\n",
    "What is the user's job and domain?\n",
    "\n",
    "### Response:\n",
    "{f}\"\"\",\n",
    "    lambda f: f\"\"\"### Instruction:\n",
    "You are an assistant that knows specific facts about one user.\n",
    "\n",
    "### Input:\n",
    "Summarize the user's background in one or two sentences.\n",
    "\n",
    "### Response:\n",
    "{f}\"\"\",\n",
    "    lambda f: f\"\"\"### Instruction:\n",
    "Use the stored personal profile of the user to answer this question.\n",
    "\n",
    "### Input:\n",
    "What kind of projects does the user usually work on?\n",
    "\n",
    "### Response:\n",
    "{f}\"\"\",\n",
    "    lambda f: f\"\"\"### Instruction:\n",
    "You are personalizing responses for this specific user.\n",
    "\n",
    "### Input:\n",
    "Describe the user's interests and profession together.\n",
    "\n",
    "### Response:\n",
    "{f}\"\"\",\n",
    "]\n",
    "\n",
    "examples = []\n",
    "for i in range(1000):\n",
    "    fact = random.choice(base_facts)\n",
    "    template = random.choice(qa_templates)\n",
    "    examples.append({\"text\": template(fact)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65ef1e-1168-4e8c-ae50-b20853007074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample input\n",
    "examples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a392bf56-7ea4-4297-87dc-5b5d5025ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a HF Dataset\n",
    "dataset = Dataset.from_list(examples).train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2970f4-ade3-4ce1-abaf-10b37dbc098d",
   "metadata": {},
   "source": [
    "## Tokenization & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf16ebe-963a-43a7-b4ee-238a3307ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Tokenizer\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf83008-3df6-4802-b211-2621fd91e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. 4-bit quantization (QLoRA)\n",
    "# -----------------------------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8470f-e9bf-453d-bce4-dc37353fe9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Load base model in 4-bit + prep for k-bit training\n",
    "# -----------------------------\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971f778-4acc-497e-8de9-643af3d28775",
   "metadata": {},
   "source": [
    "## LoRA & SFT Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca6b83-0885-45fc-8011-7a89749436ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. LoRA config: https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Llama-style\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfbcfda-2e16-42f0-9da9-a79f86774438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Define ALL configuration parameters within SFTConfig\n",
    "config = SFTConfig(\n",
    "    # --- Standard TrainingArguments parameters ---\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # --- SFT-specific parameters (moved here from previous iterations) ---\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=512,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer, passing the SFTConfig object to the 'args' parameter\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=config,  # Pass the combined SFTConfig object here\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    # No other config parameters needed here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b8280-4297-4ae0-8fc6-cb925142dfa9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be54da1-04bb-4586-bf7a-b86ef86c6d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c5bfdc-6c26-4e6d-8082-9d755b4ac076",
   "metadata": {},
   "source": [
    "## Save Adapter Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc9ce2-f7f5-4b77-aa30-becdfee0bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Save adapter weights\n",
    "# -----------------------------\n",
    "os.makedirs(ADAPTER_DIR, exist_ok=True)\n",
    "trainer.model.save_pretrained(ADAPTER_DIR)\n",
    "tokenizer.save_pretrained(ADAPTER_DIR)\n",
    "\n",
    "print(f\"Adapter saved to: {ADAPTER_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ddff21-5295-4a26-ac75-fa8d1747bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_DIR = \"./qlora-peft-output/adapter\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb2641-2899-4d32-b240-dfc58ad4ce13",
   "metadata": {},
   "source": [
    "## Inference with Base Model & Merged Model (with Adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609711f-0a3e-4d54-b93f-cbc0af4cab5a",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f358bbb2-ff20-48fb-9965-1d5da1517e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",          # puts on GPU if available\n",
    "    torch_dtype=torch.float16,  # safer for VRAM than fp32\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"What types of projects do I usually work on?\",\n",
    "    \"Where do I live and what city am I based in?\",\n",
    "    \"What sports or hobbies am I known for?\",\n",
    "    \"Summarize my background and personal interests.\",\n",
    "    \"Based on your knowledge of me, what is my professional expertise?\",\n",
    "]\n",
    "\n",
    "def generate(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            do_sample=False\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n================ BASE MODEL INFERENCE ================\\n\")\n",
    "\n",
    "for p in prompts:\n",
    "    wrapped = (\n",
    "        \"You are a helpful assistant. The user is asking a question about their personal background.\\n\"\n",
    "        \"If you do not know the answer, say so clearly.\\n\\n\"\n",
    "        f\"Question: {p}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    print(\"PROMPT:\", p)\n",
    "    result = generate(wrapped)\n",
    "    print(result)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22db023-a3bf-4a15-8bda-e40f11ac0b2b",
   "metadata": {},
   "source": [
    "### Merged Model\n",
    "We merge the adapter to the base model, you can run more proper evaluations but for POC we'll just manually see the difference in knowledge for some of these prompts here.\n",
    "\n",
    "Merging: https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.merge_and_unload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02838d-94e9-49a3-a9f5-b7f87a07d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, AutoPeftModelForCausalLM\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Paths\n",
    "# -------------------------------------------------------\n",
    "BASE_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "ADAPTER_DIR = \"./qlora-peft-output/adapter\"\n",
    "MERGED_DIR = \"./merged-model\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Load tokenizer\n",
    "# -------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_ID,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Load base model in fp16/bf16\n",
    "# (merged model will be full precision LLM)\n",
    "# -------------------------------------------------------\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Attach adapter\n",
    "# -------------------------------------------------------\n",
    "print(\"Loading adapter onto base model...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ADAPTER_DIR,\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Merge adapter â†’ base model\n",
    "# -------------------------------------------------------\n",
    "print(\"Merging LoRA adapter into base model weights...\")\n",
    "merged_model = model.merge_and_unload()   # <-- key line\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Save merged full model (optional)\n",
    "# -------------------------------------------------------\n",
    "merged_model.save_pretrained(MERGED_DIR)\n",
    "tokenizer.save_pretrained(MERGED_DIR)\n",
    "\n",
    "print(f\"\\nMerged model saved to: {MERGED_DIR}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e314ca-89ec-4f9c-b42c-4e4ee1769a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(merged_model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = merged_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Run merged-model inference\n",
    "# -------------------------------------------------------\n",
    "print(\"==================== MERGED MODEL OUTPUT ====================\\n\")\n",
    "\n",
    "for p in prompts:\n",
    "    wrapped = (\n",
    "        \"You are a personalized assistant that knows details about the user based \"\n",
    "        \"on prior fine-tuning data.\\n\\n\"\n",
    "        f\"Question: {p}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    print(f\"PROMPT: {p}\\n\")\n",
    "    output = generate(wrapped)\n",
    "    print(output)\n",
    "    print(\"-\" * 120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
