{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7709960f-7361-4053-bcaf-0a1c88523136",
   "metadata": {},
   "source": [
    "# Fine-Tuning HuggingFace Models Using Amazon SageMaker\n",
    "In this example we take a look at how we can take a sample [BERT model](https://huggingface.co/google-bert/bert-base-cased) and fine-tune it using SageMaker Training Jobs. With SageMaker Training Jobs, containers and infra layer becomes managed and also stitches in nicely with deploying to an endpoint.\n",
    "\n",
    "## Additional Resources/Credits\n",
    "- Learned a lot from PhilSchmid's example: https://github.com/huggingface/notebooks/tree/main/sagemaker/01_getting_started_pytorch\n",
    "- Trainer Documentation: https://huggingface.co/docs/transformers/en/main_classes/trainer\n",
    "- <b>NOTE</b> -> Checkpoints SM: https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints-enable.html, in this example we save checkpoints as part of the model data which I wouldn't recommend (adds to model tarball size). Refer to these docs to decouple the checkpoints to another channel.\n",
    "\n",
    "\n",
    "## Setup\n",
    "Working on a conda_python3 kernel, you can utilize any base instance here for the most part as the infra needed for training will be supplied via the Training Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b691c1-0b01-4179-98cc-0cdc4e2b6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker datasets evaluate transformers[torch] accelerate>=0.26.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33599c2d-851a-4df1-b561-ca748a7e8257",
   "metadata": {},
   "source": [
    "## Config Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35bfbd-6a8c-4831-9dec-40521b782e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()  # works in SageMaker Notebook/Studio\n",
    "\n",
    "bucket = session.default_bucket()      # or set your own bucket name\n",
    "prefix = \"bert-intro-imdb\"             # S3 prefix for this job\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "local_train_csv = \"data/train.csv\"\n",
    "local_test_csv = \"data/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94895f5e-4d9a-4ad5-aabc-51b9a7790790",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "We push dataset into S3 as a CSV, we conduct tokenization within the training script, but it can also happen on the client side depending on what you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8173cfbf-dba6-49df-b0cd-15e9f1e2440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")  # already has train / test splits\n",
    "\n",
    "train_df = dataset[\"train\"].to_pandas()\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "\n",
    "# keep columns \"text\" and \"label\" as your script expects\n",
    "train_df[[\"text\", \"label\"]].to_csv(local_train_csv, index=False)\n",
    "test_df[[\"text\", \"label\"]].to_csv(local_test_csv, index=False)\n",
    "\n",
    "print(\"Wrote train.csv and test.csv from HF IMDb dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10d249-72a8-4ef7-b9df-7f72fea86965",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "s3.upload_file(local_train_csv, bucket, f\"{prefix}/train/train.csv\")\n",
    "s3.upload_file(local_test_csv,  bucket, f\"{prefix}/test/test.csv\")\n",
    "\n",
    "s3_train = f\"s3://{bucket}/{prefix}/train\"\n",
    "s3_test  = f\"s3://{bucket}/{prefix}/test\"\n",
    "\n",
    "print(\"Uploaded to:\")\n",
    "print(\"  \", s3_train)\n",
    "print(\"  \", s3_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04843a5d-17de-48de-9204-570e5346e4a4",
   "metadata": {},
   "source": [
    "## Define HuggingFace Estimator\n",
    "With this estimator we automatically pull the needed training container by specifying transformers and torch version. You can also specify other hyperparameters, we keep pretty minimal in this case, as you get more advanced for distributed training for example you specify it in this map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce67bf9-037d-4b8f-a851-d1d1fc27c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"model_name\": \"bert-base-cased\",\n",
    "    \"num_train_epochs\": 1,\n",
    "}\n",
    "\n",
    "estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",           # your script\n",
    "    source_dir=\"./scripts\",                   # folder containing train.py\n",
    "    instance_type=\"ml.g5.12xlarge\",   # or g5/g6 etc\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version=\"4.46\",\n",
    "    pytorch_version=\"2.3\",\n",
    "    py_version=\"py311\",\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad9c11-5e32-4c0f-8ca2-4bc5ecd9d739",
   "metadata": {},
   "source": [
    "## Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362bd6bd-e2e0-40f3-83a2-75d8ed795127",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({\n",
    "    \"train\": s3_train,   # -> SM_CHANNEL_TRAIN\n",
    "    \"test\":  s3_test,    # -> SM_CHANNEL_TEST\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1962f-3ea8-4fe1-94b5-68a4c431860f",
   "metadata": {},
   "source": [
    "## Extract Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10be41-7088-466a-964f-f6da77c412fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ea2db-129b-41e9-bfe0-32b1662c22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "model_s3_uri = estimator.model_data   # e.g., s3://my-bucket/path/to/model.tar.gz\n",
    "\n",
    "parsed = urlparse(model_s3_uri)\n",
    "bucket = parsed.netloc\n",
    "key = parsed.path.lstrip(\"/\")   # remove leading \"/\"\n",
    "\n",
    "import boto3\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(bucket, key, \"model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4659f65-e516-4476-987a-9744d30fe29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open(\"model.tar.gz\") as tar:\n",
    "    tar.extractall(\"model_dir\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
